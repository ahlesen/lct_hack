{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05460ea9-5771-4db1-ab84-8651ddd3cd07",
   "metadata": {
    "id": "1ea61ff0-620e-4105-b4ac-e8d417a0454f"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm_notebook\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dabd186-e374-4bff-b4b7-4ff60e4c268d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ed47bce-65e1-47cf-b2a7-7022781e900d",
    "outputId": "dc9b78c2-9677-409f-fa3f-0efcb4cb90bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " 'requirements.txt',\n",
       " 'synonyms',\n",
       " 'yappy_search',\n",
       " '.gitignore',\n",
       " '.git',\n",
       " 'notebooks',\n",
       " '.ipynb_checkpoints']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d010021-b0dc-4960-9d6c-5ac8d9e6f169",
   "metadata": {
    "id": "b6855342-2441-4302-b1d4-a7867647230a"
   },
   "outputs": [],
   "source": [
    "# –°—á–∏—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã (CSV –∏–ª–∏ Excel, –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ñ–æ—Ä–º–∞—Ç–∞)\n",
    "file_path = '../../yappy_hackaton_2024_400k.csv'  # —É–∫–∞–∂–∏—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É\n",
    "df = pd.read_csv(file_path)  # –µ—Å–ª–∏ —É –≤–∞—Å Excel, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d5aa5bb-45c7-466b-8c6f-30db0c1b8d68",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "7e548c19-e22c-4c69-9d7f-0b5d17ccd15f",
    "outputId": "fd6f0b62-ea30-40f9-c2a6-053cae19e618"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://cdn-st.rutubelist.ru/media/b0/e9/ef285...</td>\n",
       "      <td>#–Ω–∞—Ä–µ–∑–∫–∏—Å—Ç—Ä–∏–º–æ–≤ , #dota2 , #cs2 , #fifa23 , #m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://cdn-st.rutubelist.ru/media/39/6c/b31bc...</td>\n",
       "      <td>ü§´–ù–ï –í–í–û–î–ò –≠–¢–£ –ö–û–ú–ê–ù–î–£ –í –†–û–ë–õ–û–ö–° ! #shorts #rob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://cdn-st.rutubelist.ru/media/e9/e0/b47a9...</td>\n",
       "      <td>#boobs , #–∫—Ä–∞—Å–∏–≤—ã–µ–¥–µ–≤—É—à–∫–∏ , #ass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://cdn-st.rutubelist.ru/media/87/43/b11df...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://cdn-st.rutubelist.ru/media/d1/e7/642dc...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399995</th>\n",
       "      <td>https://cdn-st.rutubelist.ru/media/dd/6a/ac296...</td>\n",
       "      <td>#ceramics #handmade #ceramic #clay #ceramicscu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399996</th>\n",
       "      <td>https://cdn-st.rutubelist.ru/media/aa/d3/4b4c3...</td>\n",
       "      <td>#–∫—Ä–∞—Å–∏–≤—ã–µ–¥–µ–≤—É—à–∫–∏ #–∫—Ä–∞—Å–æ—Ç–∫–∏ #–≥—Ä—É–¥—å #boobs #—Å–µ–∫—Å...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399997</th>\n",
       "      <td>https://cdn-st.rutubelist.ru/media/73/05/3f80f...</td>\n",
       "      <td>#–ª–∞–π—Ñ—Ö–∞–∫–∏ , #—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã , #roblox , #–∏–≥—Ä—É—à–∫–∏...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399998</th>\n",
       "      <td>https://cdn-st.rutubelist.ru/media/f8/2b/320f7...</td>\n",
       "      <td>#–æ–±—Ä–∞–∑ #lookbook #–ø–æ–∫–∞–∑ #–Ω–µ–¥–µ–ª—è–º–æ–¥—ã #–∫–∞–ø—Å—É–ª–∞ #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399999</th>\n",
       "      <td>https://cdn-st.rutubelist.ru/media/19/1b/5711a...</td>\n",
       "      <td>#–æ–±—Ä–∞–∑ #lookbook #–ø–æ–∫–∞–∑ #–Ω–µ–¥–µ–ª—è–º–æ–¥—ã #–∫–∞–ø—Å—É–ª–∞ #...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400000 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     link  \\\n",
       "0       https://cdn-st.rutubelist.ru/media/b0/e9/ef285...   \n",
       "1       https://cdn-st.rutubelist.ru/media/39/6c/b31bc...   \n",
       "2       https://cdn-st.rutubelist.ru/media/e9/e0/b47a9...   \n",
       "3       https://cdn-st.rutubelist.ru/media/87/43/b11df...   \n",
       "4       https://cdn-st.rutubelist.ru/media/d1/e7/642dc...   \n",
       "...                                                   ...   \n",
       "399995  https://cdn-st.rutubelist.ru/media/dd/6a/ac296...   \n",
       "399996  https://cdn-st.rutubelist.ru/media/aa/d3/4b4c3...   \n",
       "399997  https://cdn-st.rutubelist.ru/media/73/05/3f80f...   \n",
       "399998  https://cdn-st.rutubelist.ru/media/f8/2b/320f7...   \n",
       "399999  https://cdn-st.rutubelist.ru/media/19/1b/5711a...   \n",
       "\n",
       "                                              description  \n",
       "0       #–Ω–∞—Ä–µ–∑–∫–∏—Å—Ç—Ä–∏–º–æ–≤ , #dota2 , #cs2 , #fifa23 , #m...  \n",
       "1       ü§´–ù–ï –í–í–û–î–ò –≠–¢–£ –ö–û–ú–ê–ù–î–£ –í –†–û–ë–õ–û–ö–° ! #shorts #rob...  \n",
       "2                        #boobs , #–∫—Ä–∞—Å–∏–≤—ã–µ–¥–µ–≤—É—à–∫–∏ , #ass  \n",
       "3                                                     NaN  \n",
       "4                                                     NaN  \n",
       "...                                                   ...  \n",
       "399995  #ceramics #handmade #ceramic #clay #ceramicscu...  \n",
       "399996  #–∫—Ä–∞—Å–∏–≤—ã–µ–¥–µ–≤—É—à–∫–∏ #–∫—Ä–∞—Å–æ—Ç–∫–∏ #–≥—Ä—É–¥—å #boobs #—Å–µ–∫—Å...  \n",
       "399997  #–ª–∞–π—Ñ—Ö–∞–∫–∏ , #—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã , #roblox , #–∏–≥—Ä—É—à–∫–∏...  \n",
       "399998  #–æ–±—Ä–∞–∑ #lookbook #–ø–æ–∫–∞–∑ #–Ω–µ–¥–µ–ª—è–º–æ–¥—ã #–∫–∞–ø—Å—É–ª–∞ #...  \n",
       "399999  #–æ–±—Ä–∞–∑ #lookbook #–ø–æ–∫–∞–∑ #–Ω–µ–¥–µ–ª—è–º–æ–¥—ã #–∫–∞–ø—Å—É–ª–∞ #...  \n",
       "\n",
       "[400000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19c3f767-4de7-4aa8-b821-b5f9ac07e735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(\n",
    "    0,\n",
    "    \"/home/jovyan/anvlasov/hack_yappy/git/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce81a8b5-8648-4788-ac71-0e64c09c8128",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_w_video:str = '../../data_video/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fed84868-3c3b-422c-9eae-4da4b05604fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data_video/0_b0_e9_ef285e0241139fc611318ed33071_fhd.mp4\n",
      "../../data_video/1_39_6c_b31bc6864bef9d8a96814f1822ca_fhd.mp4\n",
      "../../data_video/2_e9_e0_b47a9df14a5e97942715e5e705c0_fhd.mp4\n",
      "../../data_video/3_87_43_b11df3f344d0af773aac81e410ee_fhd.mp4\n",
      "../../data_video/4_d1_e7_642dc2194fcdb69664f832d5f2dd_fhd.mp4\n"
     ]
    }
   ],
   "source": [
    "# –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º .mp4 –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏\n",
    "video_files = glob.glob(os.path.join(dir_w_video, '*.mp4'))\n",
    "# –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ —Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤ –ø–æ –ø–µ—Ä–≤–æ–º—É –∑–Ω–∞—á–µ–Ω–∏—é –ø—Ä–∏ split('_')\n",
    "sorted_video_files = sorted(video_files, key=lambda x: int(os.path.basename(x).split('_')[0]))\n",
    "\n",
    "# –í—ã–≤–æ–¥ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤\n",
    "for video_file in sorted_video_files[:5]:\n",
    "    print(video_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3221e876-97df-4ddf-8036-e29a4ebf7211",
   "metadata": {},
   "source": [
    "### –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –ø–æ –ø–æ–¥–±–æ—Ä—É –º–æ–¥–µ–ª–∏ whisper –∏ –∑–∞–ø—É—Å–∫—É –Ω–∞ batch\n",
    "\n",
    "—Å–º–æ—Ç—Ä–∏–º –Ω–∞ 100 –ø—Ä–∏–º–µ—Ä–∞—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c998da51-cdaf-40a7-a722-cb47b2979067",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yappy_search.utils import  extract_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e88efa4-7d98-43ff-bfff-c0d409e8d9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = sorted_video_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be235b66-0d8e-4d60-b5f7-5d8da7150866",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_output_dir:str= os.path.join(video_path.rsplit('/',2)[0],'data_audio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbc88397-8e05-4191-8f24-83850f9c16f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../data_audio'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4812dad7-b85a-4519-8f0f-6de19beae08c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6be2fae9fe9449c8b47a80fd0d81350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "audio_paths = []\n",
    "for video_path in tqdm_notebook(sorted_video_files[:100]):\n",
    "    audio_file_path = os.path.join(\n",
    "        audio_output_dir, os.path.basename(video_path).replace(\".mp4\", \".mp3\")\n",
    "    )\n",
    "    if os.path.isfile(audio_file_path):\n",
    "        pass\n",
    "    else:\n",
    "        audio_file_path = extract_audio(video_path, audio_output_dir)\n",
    "    audio_paths.append(audio_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84898c7-de7c-4c71-ab72-e1a36b9445be",
   "metadata": {},
   "source": [
    "### –Ω–∞–ø—Ä—è–º—É—é –ø–æ–ø—Ä–æ–±—É–µ–º —É—Å–∫–æ—Ä–∏—Ç—å —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "88b5f9fe-b9a4-4cc6-9a39-79906d359097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ac282d32-0f56-4d94-9fe6-1f80855a7498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e91fed3-6aa5-44d7-9d1e-da636c37d6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "72ff584a-0379-437d-8716-ce07009062bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_song = 100\n",
    "batch_size=32\n",
    "language= 'ru'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6c9955b1-175b-4cb7-9a01-0c07302beb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    chunk_length_s=30,\n",
    "    batch_size=batch_size,\n",
    "    return_timestamps=True,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "06f5f154-f3a3-4af7-8735-a4fb77a8f8a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79de715d8af841c485e68eb26543f5b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m audio_path \u001b[38;5;129;01min\u001b[39;00m tqdm_notebook(audio_paths[:limit_song]):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# audio_path = model_all.audio_transcription.extract_audio(video_path, audio_output_dir)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     transcription \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     transcriptions\u001b[38;5;241m.\u001b[39mappend(transcription)\n\u001b[1;32m      8\u001b[0m time_work \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/pipelines/automatic_speech_recognition.py:285\u001b[0m, in \u001b[0;36mAutomaticSpeechRecognitionPipeline.__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    224\u001b[0m     inputs: Union[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    226\u001b[0m ):\n\u001b[1;32m    227\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m    Transcribe the audio sequence(s) given as inputs to text. See the [`AutomaticSpeechRecognitionPipeline`]\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m    documentation for more information.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;124;03m                `\"\".join(chunk[\"text\"] for chunk in output[\"chunks\"])`.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/pipelines/base.py:1235\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1233\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[1;32m   1234\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ChunkPipeline):\n\u001b[0;32m-> 1235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1238\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/pipelines/pt_utils.py:269\u001b[0m, in \u001b[0;36mPipelinePackIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[0;32m--> 269\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/pipelines/base.py:1150\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1149\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1150\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1151\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/pipelines/automatic_speech_recognition.py:506\u001b[0m, in \u001b[0;36mAutomaticSpeechRecognitionPipeline._forward\u001b[0;34m(self, model_inputs, return_timestamps, **generate_kwargs)\u001b[0m\n\u001b[1;32m    504\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_features\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m inputs\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 506\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    509\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgenerate_kwargs,\n\u001b[1;32m    511\u001b[0m )\n\u001b[1;32m    512\u001b[0m \u001b[38;5;66;03m# whisper longform generation stores timestamps in \"segments\"\u001b[39;00m\n",
      "File \u001b[0;32m~/.mlspace/envs/anvlasov/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.mlspace/envs/anvlasov/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/whisper/modeling_whisper.py:1209\u001b[0m, in \u001b[0;36mWhisperEncoder.forward\u001b[0;34m(self, input_features, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1201\u001b[0m         layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1202\u001b[0m             encoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1203\u001b[0m             hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1206\u001b[0m             output_attentions,\n\u001b[1;32m   1207\u001b[0m         )\n\u001b[1;32m   1208\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1209\u001b[0m         layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1211\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1212\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1213\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1214\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1216\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/.mlspace/envs/anvlasov/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.mlspace/envs/anvlasov/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/whisper/modeling_whisper.py:765\u001b[0m, in \u001b[0;36mWhisperEncoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    763\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    764\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn_layer_norm(hidden_states)\n\u001b[0;32m--> 765\u001b[0m hidden_states, attn_weights, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    766\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    767\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    769\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    771\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m    772\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/.mlspace/envs/anvlasov/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.mlspace/envs/anvlasov/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/whisper/modeling_whisper.py:687\u001b[0m, in \u001b[0;36mWhisperSdpaAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_decoder:\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;66;03m# if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# Further calls to cross_attention layer can then reuse all cross-attention\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    683\u001b[0m     \u001b[38;5;66;03m# can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\u001b[39;00m\n\u001b[1;32m    684\u001b[0m     \u001b[38;5;66;03m# if encoder bi-directional self-attention `past_key_value` is always `None`\u001b[39;00m\n\u001b[1;32m    685\u001b[0m     past_key_value \u001b[38;5;241m=\u001b[39m (key_states, value_states)\n\u001b[0;32m--> 687\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;66;03m# NOTE: SDPA with memory-efficient backend is currently (torch==2.1.2) bugged when using non-contiguous inputs and a custom attn_mask,\u001b[39;00m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;66;03m# but we are fine here as `_shape` do call `.contiguous()`. Reference: https://github.com/pytorch/pytorch/issues/112577\u001b[39;00m\n\u001b[1;32m    691\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mscaled_dot_product_attention(\n\u001b[1;32m    692\u001b[0m     query_states,\n\u001b[1;32m    693\u001b[0m     key_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    698\u001b[0m     is_causal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_causal \u001b[38;5;129;01mand\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tgt_len \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    699\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/whisper/modeling_whisper.py:271\u001b[0m, in \u001b[0;36mWhisperAttention._shape\u001b[0;34m(self, tensor, seq_len, bsz)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_shape\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: torch\u001b[38;5;241m.\u001b[39mTensor, seq_len: \u001b[38;5;28mint\u001b[39m, bsz: \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "transcriptions = []\n",
    "start_time = time.time()\n",
    "for audio_path in tqdm_notebook(audio_paths[:limit_song]):\n",
    "    # audio_path = model_all.audio_transcription.extract_audio(video_path, audio_output_dir)\n",
    "    transcription = pipe(audio_path)\n",
    "\n",
    "    transcriptions.append(transcription)\n",
    "time_work = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f3dad420-58aa-4732-b48a-09794c3152d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8525820064544678 –≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã –Ω–∞ 1 –∑–∞–ø–∏—Å–∏ –≤ —Å—Ä–µ–¥–Ω–µ–º\n"
     ]
    }
   ],
   "source": [
    "print(f\"{time_work/limit_song} –≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã –Ω–∞ 1 –∑–∞–ø–∏—Å–∏ –≤ —Å—Ä–µ–¥–Ω–µ–º\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3b2306a0-f43c-41cb-8988-96db7eab7a12",
   "metadata": {},
   "source": [
    "1.61 –≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã –Ω–∞ 1 –∑–∞–ø–∏—Å–∏ –≤ —Å—Ä–µ–¥–Ω–µ–º - **batch_size=16**\n",
    "1.85 –≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã –Ω–∞ 1 –∑–∞–ø–∏—Å–∏ –≤ —Å—Ä–µ–¥–Ω–µ–º - **batch_size=32**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e523ecf9-1cd9-44bd-8f76-cb2a63592435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' –°–µ–π—á–∞—Å —è —Ç–µ–±–µ –ø–æ–∫–∞–∂—É —Å–µ–∫—Ä–µ—Ç–Ω—É—é –∫–æ–º–∞–Ω–¥—É –≤ –†–æ–±–ª–æ–∫—Å–µ. –ß—Ç–æ–±—ã –µ—ë –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å –ø–æ—Å—Ç–∞–≤—å –ª–∞–π–∫ –∏ –ø–æ–¥–ø–∏—à–∏—Å—å, –∞ —Ç–∞–∫–∂–µ –≤–≤–µ–¥–∏ –≤ —á–∞—Ç –∫–æ–º–∞–Ω–¥—É ILOVEYOU. –ö–æ–≥–¥–∞ –≤—ã –µ—ë –≤–≤–µ–¥—ë—Ç–µ —É –≤–∞—Å –Ω–∞ —ç–∫—Ä–∞–Ω–µ –ø–æ—è–≤–∏—Ç—Å—è –≤–æ—Ç —Ç–∞–∫–æ–π –∫–ª–æ—É–Ω. –°–∫—Ä–∏–ø—Ç –±—ã–ª —Å–æ–∑–¥–∞–Ω –æ–¥–Ω–∏–º –∏–∑ —Å–æ–∑–¥–∞—Ç–µ–ª–µ–π –†–æ–±–ª–æ–∫—Å–∞ –∏ —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ –≤ –∏–≥—Ä–∞—Ö, –≥–¥–µ –µ—Å—Ç—å –†–æ–±–ª–æ—Å –¥–µ–≤–µ–ª–æ–ø–µ—Ä —Å–µ—Ä–≤–∏—Å. –ß—Ç–æ–±—ã —É–±—Ä–∞—Ç—å —Å–∫—Ä–∏–º–µ—Ä –Ω—É–∂–Ω–æ –≤—Å–µ–≥–æ',\n",
       " 'chunks': [{'timestamp': (0.0, 2.56),\n",
       "   'text': ' –°–µ–π—á–∞—Å —è —Ç–µ–±–µ –ø–æ–∫–∞–∂—É —Å–µ–∫—Ä–µ—Ç–Ω—É—é –∫–æ–º–∞–Ω–¥—É –≤ –†–æ–±–ª–æ–∫—Å–µ.'},\n",
       "  {'timestamp': (2.56, 4.88),\n",
       "   'text': ' –ß—Ç–æ–±—ã –µ—ë –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å –ø–æ—Å—Ç–∞–≤—å –ª–∞–π–∫ –∏ –ø–æ–¥–ø–∏—à–∏—Å—å, –∞ —Ç–∞–∫–∂–µ'},\n",
       "  {'timestamp': (4.88, 6.8), 'text': ' –≤–≤–µ–¥–∏ –≤ —á–∞—Ç –∫–æ–º–∞–Ω–¥—É ILOVEYOU.'},\n",
       "  {'timestamp': (6.8, 9.12),\n",
       "   'text': ' –ö–æ–≥–¥–∞ –≤—ã –µ—ë –≤–≤–µ–¥—ë—Ç–µ —É –≤–∞—Å –Ω–∞ —ç–∫—Ä–∞–Ω–µ –ø–æ—è–≤–∏—Ç—Å—è –≤–æ—Ç'},\n",
       "  {'timestamp': (9.12, 10.46), 'text': ' —Ç–∞–∫–æ–π –∫–ª–æ—É–Ω.'},\n",
       "  {'timestamp': (10.46, 12.2),\n",
       "   'text': ' –°–∫—Ä–∏–ø—Ç –±—ã–ª —Å–æ–∑–¥–∞–Ω –æ–¥–Ω–∏–º –∏–∑ —Å–æ–∑–¥–∞—Ç–µ–ª–µ–π –†–æ–±–ª–æ–∫—Å–∞'},\n",
       "  {'timestamp': (12.2, 14.6),\n",
       "   'text': ' –∏ —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ –≤ –∏–≥—Ä–∞—Ö, –≥–¥–µ –µ—Å—Ç—å –†–æ–±–ª–æ—Å –¥–µ–≤–µ–ª–æ–ø–µ—Ä'},\n",
       "  {'timestamp': (14.6, 15.6), 'text': ' —Å–µ—Ä–≤–∏—Å.'},\n",
       "  {'timestamp': (15.6, None), 'text': ' –ß—Ç–æ–±—ã —É–±—Ä–∞—Ç—å —Å–∫—Ä–∏–º–µ—Ä –Ω—É–∂–Ω–æ –≤—Å–µ–≥–æ'}]}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcriptions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cfec4a85-29da-4bd3-a557-96c553d44959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' –°–µ–π—á–∞—Å —è —Ç–µ–±–µ –ø–æ–∫–∞–∂—É —Å–µ–∫—Ä–µ—Ç–Ω—É—é –∫–æ–º–∞–Ω–¥—É –≤ –†–æ–±–ª–æ–∫—Å–µ. –ß—Ç–æ–±—ã –µ—ë –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å –ø–æ—Å—Ç–∞–≤—å –ª–∞–π–∫ –∏ –ø–æ–¥–ø–∏—à–∏—Å—å, –∞ —Ç–∞–∫–∂–µ –≤–≤–µ–¥–∏ –≤ —á–∞—Ç –∫–æ–º–∞–Ω–¥—É ILOVEYOU. –ö–æ–≥–¥–∞ –≤—ã –µ—ë –≤–≤–µ–¥—ë—Ç–µ —É –≤–∞—Å –Ω–∞ —ç–∫—Ä–∞–Ω–µ –ø–æ—è–≤–∏—Ç—Å—è –≤–æ—Ç —Ç–∞–∫–æ–π –∫–ª–æ—É–Ω. –°–∫—Ä–∏–ø—Ç –±—ã–ª —Å–æ–∑–¥–∞–Ω –æ–¥–Ω–∏–º –∏–∑ —Å–æ–∑–¥–∞—Ç–µ–ª–µ–π –†–æ–±–ª–æ–∫—Å–∞ –∏ —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ –≤ –∏–≥—Ä–∞—Ö, –≥–¥–µ –µ—Å—Ç—å –†–æ–±–ª–æ—Å –¥–µ–≤–µ–ª–æ–ø–µ—Ä —Å–µ—Ä–≤–∏—Å. –ß—Ç–æ–±—ã —É–±—Ä–∞—Ç—å —Å–∫—Ä–∏–º–µ—Ä –Ω—É–∂–Ω–æ –≤—Å–µ–≥–æ',\n",
       " 'chunks': [{'timestamp': (0.0, 2.56),\n",
       "   'text': ' –°–µ–π—á–∞—Å —è —Ç–µ–±–µ –ø–æ–∫–∞–∂—É —Å–µ–∫—Ä–µ—Ç–Ω—É—é –∫–æ–º–∞–Ω–¥—É –≤ –†–æ–±–ª–æ–∫—Å–µ.'},\n",
       "  {'timestamp': (2.56, 4.88),\n",
       "   'text': ' –ß—Ç–æ–±—ã –µ—ë –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å –ø–æ—Å—Ç–∞–≤—å –ª–∞–π–∫ –∏ –ø–æ–¥–ø–∏—à–∏—Å—å, –∞ —Ç–∞–∫–∂–µ'},\n",
       "  {'timestamp': (4.88, 6.8), 'text': ' –≤–≤–µ–¥–∏ –≤ —á–∞—Ç –∫–æ–º–∞–Ω–¥—É ILOVEYOU.'},\n",
       "  {'timestamp': (6.8, 9.12),\n",
       "   'text': ' –ö–æ–≥–¥–∞ –≤—ã –µ—ë –≤–≤–µ–¥—ë—Ç–µ —É –≤–∞—Å –Ω–∞ —ç–∫—Ä–∞–Ω–µ –ø–æ—è–≤–∏—Ç—Å—è –≤–æ—Ç'},\n",
       "  {'timestamp': (9.12, 10.46), 'text': ' —Ç–∞–∫–æ–π –∫–ª–æ—É–Ω.'},\n",
       "  {'timestamp': (10.46, 12.2),\n",
       "   'text': ' –°–∫—Ä–∏–ø—Ç –±—ã–ª —Å–æ–∑–¥–∞–Ω –æ–¥–Ω–∏–º –∏–∑ —Å–æ–∑–¥–∞—Ç–µ–ª–µ–π –†–æ–±–ª–æ–∫—Å–∞'},\n",
       "  {'timestamp': (12.2, 14.6),\n",
       "   'text': ' –∏ —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ –≤ –∏–≥—Ä–∞—Ö, –≥–¥–µ –µ—Å—Ç—å –†–æ–±–ª–æ—Å –¥–µ–≤–µ–ª–æ–ø–µ—Ä'},\n",
       "  {'timestamp': (14.6, 15.6), 'text': ' —Å–µ—Ä–≤–∏—Å.'},\n",
       "  {'timestamp': (15.6, None), 'text': ' –ß—Ç–æ–±—ã —É–±—Ä–∞—Ç—å —Å–∫—Ä–∏–º–µ—Ä –Ω—É–∂–Ω–æ –≤—Å–µ–≥–æ'}]}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcriptions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "01084d64-5f24-46e3-b544-50c0ad4158fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' –°—É–±—Ç–∏—Ç—Ä—ã —Å–¥–µ–ª–∞–ª DimaTorzok',\n",
       " 'chunks': [{'timestamp': (0.0, 10.94),\n",
       "   'text': ' –°—É–±—Ç–∏—Ç—Ä—ã —Å–¥–µ–ª–∞–ª DimaTorzok'}]}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcriptions[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ee4a0b09-f3f3-4634-a906-349d5584901b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' –ß—Ç–æ –æ–Ω –æ —Ç–µ–±–µ –¥—É–º–∞–µ—Ç –ø—Ä—è–º–æ —Å–µ–π—á–∞—Å? –°–ª—É—à–∞–π, –¥—É–º–∞–µ—Ç, —á—Ç–æ —Ç—ã, –∑–Ω–∞–µ—à—å, –¥–ª—è –Ω–µ–≥–æ –Ω–µ–ø–æ—Å–∏–ª—å–Ω–∞—è –Ω–æ—à–∞. –ö–∞–∫ –±—É–¥—Ç–æ –±—ã, –¥–∞, –∫–æ—Ä–æ–ª–µ–≤–∞ –º–µ—á–µ–π. –ß—É–≤—Å—Ç–≤—É–µ—Ç —Ç–æ, —á—Ç–æ —Ç—ã –ø–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä—É –≥–æ—Ä–∞–∑–¥–æ —Å–∏–ª—å–Ω–µ–µ, –∏ —Ç–æ, —á—Ç–æ –µ–º—É —Å —Ç–æ–±–æ–π –±—É–¥–µ—Ç –æ—á–µ–Ω—å —Å–ª–æ–∂–Ω–æ. –ù–æ –ø—Ä–∏ —ç—Ç–æ–º –µ—Å—Ç—å –∂–µ–ª–∞–Ω–∏—è, –∏ –∂–µ–ª–∞–Ω–∏—è –Ω–µ —Å–æ–≤—Å–µ–º –ø—Ä–∏–ª–∏—á–Ω—ã–µ. –ü—Ä–æ—è–≤–ª—è—Ç—å—Å—è –±–æ–∏—Ç—Å—è, –∏, —á–µ—Å—Ç–Ω–æ –≥–æ–≤–æ—Ä—è, —Å–∞–º –ø—Ä–æ—è–≤–ª—è—Ç—å—Å—è –Ω–µ –±—É–¥–µ—Ç. –Ø –±—ã –Ω–∞ —Ç–≤–æ–µ–º –º–µ—Å—Ç–µ –≤–æ–æ–±—â–µ –ø–æ–¥—É–º–∞–ª–∞, –Ω—É–∂–µ–Ω –ª–∏ —Ç–µ–±–µ —Ç–∞–∫–æ–π –ø–∞—Ä—Ç–Ω–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π —Å–ª–∞–±–µ–µ —Ç–µ–±—è. –ù–æ –≤—ã–≤–æ–¥—ã –¥–µ–ª–∞–π —Å–∞–º–∞.',\n",
       " 'chunks': [{'timestamp': (0.0, 2.12),\n",
       "   'text': ' –ß—Ç–æ –æ–Ω –æ —Ç–µ–±–µ –¥—É–º–∞–µ—Ç –ø—Ä—è–º–æ —Å–µ–π—á–∞—Å?'},\n",
       "  {'timestamp': (6.0, 10.22),\n",
       "   'text': ' –°–ª—É—à–∞–π, –¥—É–º–∞–µ—Ç, —á—Ç–æ —Ç—ã, –∑–Ω–∞–µ—à—å, –¥–ª—è –Ω–µ–≥–æ –Ω–µ–ø–æ—Å–∏–ª—å–Ω–∞—è –Ω–æ—à–∞.'},\n",
       "  {'timestamp': (10.88, 13.4), 'text': ' –ö–∞–∫ –±—É–¥—Ç–æ –±—ã, –¥–∞, –∫–æ—Ä–æ–ª–µ–≤–∞ –º–µ—á–µ–π.'},\n",
       "  {'timestamp': (14.04, 19.12),\n",
       "   'text': ' –ß—É–≤—Å—Ç–≤—É–µ—Ç —Ç–æ, —á—Ç–æ —Ç—ã –ø–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä—É –≥–æ—Ä–∞–∑–¥–æ —Å–∏–ª—å–Ω–µ–µ, –∏ —Ç–æ, —á—Ç–æ –µ–º—É —Å —Ç–æ–±–æ–π –±—É–¥–µ—Ç –æ—á–µ–Ω—å —Å–ª–æ–∂–Ω–æ.'},\n",
       "  {'timestamp': (19.6, 23.8),\n",
       "   'text': ' –ù–æ –ø—Ä–∏ —ç—Ç–æ–º –µ—Å—Ç—å –∂–µ–ª–∞–Ω–∏—è, –∏ –∂–µ–ª–∞–Ω–∏—è –Ω–µ —Å–æ–≤—Å–µ–º –ø—Ä–∏–ª–∏—á–Ω—ã–µ.'},\n",
       "  {'timestamp': (24.96, 28.56),\n",
       "   'text': ' –ü—Ä–æ—è–≤–ª—è—Ç—å—Å—è –±–æ–∏—Ç—Å—è, –∏, —á–µ—Å—Ç–Ω–æ –≥–æ–≤–æ—Ä—è, —Å–∞–º –ø—Ä–æ—è–≤–ª—è—Ç—å—Å—è –Ω–µ –±—É–¥–µ—Ç.'},\n",
       "  {'timestamp': (29.78, 35.3),\n",
       "   'text': ' –Ø –±—ã –Ω–∞ —Ç–≤–æ–µ–º –º–µ—Å—Ç–µ –≤–æ–æ–±—â–µ –ø–æ–¥—É–º–∞–ª–∞, –Ω—É–∂–µ–Ω –ª–∏ —Ç–µ–±–µ —Ç–∞–∫–æ–π –ø–∞—Ä—Ç–Ω–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π —Å–ª–∞–±–µ–µ —Ç–µ–±—è.'},\n",
       "  {'timestamp': (36.5, 38.32), 'text': ' –ù–æ –≤—ã–≤–æ–¥—ã –¥–µ–ª–∞–π —Å–∞–º–∞.'}]}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcriptions[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6d5b0c-4102-489a-966a-8be80ebc9572",
   "metadata": {},
   "source": [
    "| model          | GPU usage|\n",
    "|whisper-large-v3| 4784MiB  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128b254f-6558-42fd-be82-2eb13c9ac7e6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### –ü—Ä–æ–±—É–µ–º —É—Å–∫–æ—Ä–∏—Ç—å –±–∞—Ç—á–∞–º–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "715c25f3-712c-4615-aeb3-e9c1db8380a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fcceb5a9-a1e4-4966-8cdf-d62925e58b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∞—É–¥–∏–æ\n",
    "def load_audio(audio_paths):\n",
    "    audios = []\n",
    "    for path in audio_paths:\n",
    "        audio, sr = librosa.load(path, sr=16000)\n",
    "        audios.append(audio)\n",
    "    return audios\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∞—É–¥–∏–æ\n",
    "def transcribe_batch(audio_paths):\n",
    "    audios = load_audio(audio_paths)\n",
    "    inputs = processor(audios, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        transcriptions = pipe(inputs[\"input_features\"].to(torch_dtype))\n",
    "    return transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "248e4f35-3661-4f4e-a3c6-22ce8d16d31e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(audio_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "df887613-ab40-4264-9812-b1618a2c376c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1cd7ee3e-b488-4e56-b89b-7b4d8f79c097",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "We expect a numpy ndarray as input, got `<class 'torch.Tensor'>`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(audio_paths), batch_size):\n\u001b[1;32m      7\u001b[0m     batch_paths \u001b[38;5;241m=\u001b[39m audio_paths[i:i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[0;32m----> 8\u001b[0m     batch_transcriptions \u001b[38;5;241m=\u001b[39m \u001b[43mtranscribe_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_paths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     transcriptions\u001b[38;5;241m.\u001b[39mextend(batch_transcriptions)\n\u001b[1;32m     11\u001b[0m time_work \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "Cell \u001b[0;32mIn[60], line 15\u001b[0m, in \u001b[0;36mtranscribe_batch\u001b[0;34m(audio_paths)\u001b[0m\n\u001b[1;32m     13\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {key: val\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 15\u001b[0m     transcriptions \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_features\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m transcriptions\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/pipelines/automatic_speech_recognition.py:285\u001b[0m, in \u001b[0;36mAutomaticSpeechRecognitionPipeline.__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    224\u001b[0m     inputs: Union[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    226\u001b[0m ):\n\u001b[1;32m    227\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m    Transcribe the audio sequence(s) given as inputs to text. See the [`AutomaticSpeechRecognitionPipeline`]\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m    documentation for more information.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;124;03m                `\"\".join(chunk[\"text\"] for chunk in output[\"chunks\"])`.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/pipelines/base.py:1235\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1233\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[1;32m   1234\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ChunkPipeline):\n\u001b[0;32m-> 1235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1238\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/pipelines/pt_utils.py:269\u001b[0m, in \u001b[0;36mPipelinePackIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[0;32m--> 269\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[0;32m~/.mlspace/envs/anvlasov/lib/python3.9/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.mlspace/envs/anvlasov/lib/python3.9/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.mlspace/envs/anvlasov/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:32\u001b[0m, in \u001b[0;36m_IterableDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 32\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_iter\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/pipelines/pt_utils.py:186\u001b[0m, in \u001b[0;36mPipelineChunkIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubiterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;66;03m# Try to return next item\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubiterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;66;03m# When a preprocess iterator ends, we can start lookig at the next item\u001b[39;00m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;66;03m# ChunkIterator will keep feeding until ALL elements of iterator\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# Another way to look at it, is we're basically flattening lists of lists\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# into a single list, but with generators\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubiterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/pipelines/automatic_speech_recognition.py:410\u001b[0m, in \u001b[0;36mAutomaticSpeechRecognitionPipeline.preprocess\u001b[0;34m(self, inputs, chunk_length_s, stride_length_s)\u001b[0m\n\u001b[1;32m    408\u001b[0m         stride \u001b[38;5;241m=\u001b[39m (inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mround\u001b[39m(stride[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m ratio)), \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mround\u001b[39m(stride[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m ratio)))\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m--> 410\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe expect a numpy ndarray as input, got `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(inputs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inputs\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe expect a single channel audio input for AutomaticSpeechRecognitionPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: We expect a numpy ndarray as input, got `<class 'torch.Tensor'>`"
     ]
    }
   ],
   "source": [
    "# –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∞—É–¥–∏–æ—Ñ–∞–π–ª–æ–≤ –±–∞—Ç—á–∞–º–∏\n",
    "transcriptions = []\n",
    "start_time = time.time()\n",
    "\n",
    "# –†–∞–∑–±–∏–≤–∞–µ–º —Å–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –Ω–∞ –±–∞—Ç—á–∏\n",
    "for i in range(0, len(audio_paths), batch_size):\n",
    "    batch_paths = audio_paths[i:i + batch_size]\n",
    "    batch_transcriptions = transcribe_batch(batch_paths)\n",
    "    transcriptions.extend(batch_transcriptions)\n",
    "\n",
    "time_work = time.time() - start_time\n",
    "print(f\"–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {time_work:.2f} —Å–µ–∫—É–Ω–¥\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26e741e-7564-4ec1-adae-f41df97d71be",
   "metadata": {},
   "source": [
    "–Ω–µ –ø–æ–º–æ–≥–ª–æ, –∏–¥–µ–º –¥–∞–ª—å—à–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec847e88-c858-4f34-96ed-7dea906b6cb9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### –ø—Ä–æ–±–∞ –ª–∏–±—ã whisper\n",
    "\n",
    "—Ç—Ä–∞—Ç–∏—Ç 10 –ì–± –ì–ü–£, –±–∞—Ç—á –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –Ω–µ—Ç. —Å–∫–æ—Ä–æ—Å—Ç—å —Ç–∞–∂–µ \n",
    "\n",
    "–ø—Ä–æ–±–æ–≤–∞–ª –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å [—Ç—É—Ç](batch_transcriptions) –±–∞—Ç—á –ø–æ –ø—É—Ç—è–º –∫ mp3 —Ä–µ–∑—É–ª—å—Ç–∞—Ç - –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49236220-f9bb-428a-ac06-0e49da50f62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6c0afb5-3c8f-4e57-8ea9-8d90348b2530",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.88G/2.88G [00:58<00:00, 53.0MiB/s]\n"
     ]
    }
   ],
   "source": [
    "model = whisper.load_model(\"large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb7f352c-a9f8-46e8-900b-edf539b04096",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_n = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "571ad8bb-502e-49f8-9820-5349d6908e53",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected np.ndarray (got list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(audio_paths), batch_size_n):\n\u001b[1;32m      7\u001b[0m     batch_paths \u001b[38;5;241m=\u001b[39m audio_paths[i:i \u001b[38;5;241m+\u001b[39m batch_size_n]\n\u001b[0;32m----> 8\u001b[0m     batch_transcriptions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranscribe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_paths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     transcriptions\u001b[38;5;241m.\u001b[39mextend(batch_transcriptions)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/whisper/transcribe.py:122\u001b[0m, in \u001b[0;36mtranscribe\u001b[0;34m(model, audio, verbose, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, condition_on_previous_text, initial_prompt, word_timestamps, prepend_punctuations, append_punctuations, **decode_options)\u001b[0m\n\u001b[1;32m    119\u001b[0m     decode_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp16\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# Pad 30-seconds of silence to the input audio, for slicing\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m mel \u001b[38;5;241m=\u001b[39m \u001b[43mlog_mel_spectrogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdims\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_mels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_SAMPLES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m content_frames \u001b[38;5;241m=\u001b[39m mel\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m N_FRAMES\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decode_options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/whisper/audio.py:141\u001b[0m, in \u001b[0;36mlog_mel_spectrogram\u001b[0;34m(audio, n_mels, padding, device)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(audio, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    140\u001b[0m         audio \u001b[38;5;241m=\u001b[39m load_audio(audio)\n\u001b[0;32m--> 141\u001b[0m     audio \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     audio \u001b[38;5;241m=\u001b[39m audio\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected np.ndarray (got list)"
     ]
    }
   ],
   "source": [
    "# –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∞—É–¥–∏–æ—Ñ–∞–π–ª–æ–≤ –±–∞—Ç—á–∞–º–∏\n",
    "transcriptions = []\n",
    "start_time = time.time()\n",
    "\n",
    "# –†–∞–∑–±–∏–≤–∞–µ–º —Å–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –Ω–∞ –±–∞—Ç—á–∏\n",
    "for i in range(0, len(audio_paths), batch_size_n):\n",
    "    batch_paths = audio_paths[i:i + batch_size_n]\n",
    "    batch_transcriptions = model.transcribe(batch_paths)\n",
    "    transcriptions.extend(batch_transcriptions)\n",
    "    break\n",
    "time_work = time.time() - start_time\n",
    "print(f\"–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {time_work:.2f} —Å–µ–∫—É–Ω–¥\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d186291f-aefd-4df5-9555-46872228d86f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../data_audio/0_b0_e9_ef285e0241139fc611318ed33071_fhd.mp3',\n",
       " '../../data_audio/1_39_6c_b31bc6864bef9d8a96814f1822ca_fhd.mp3']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1cddd6fb-b1a6-43bf-bf36-fdab9f28daf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.72 s, sys: 195 ms, total: 7.92 s\n",
      "Wall time: 5.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch_transcriptions = model.transcribe(batch_paths[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "512e5878-8c20-4284-be08-c2da999c7992",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' –°–µ–π—á–∞—Å —è —Ç–µ–±–µ –ø–æ–∫–∞–∂—É —Å–µ–∫—Ä–µ—Ç–Ω—É—é –∫–æ–º–∞–Ω–¥—É –≤ –†–æ–±–ª–æ–∫—Å–µ. –ß—Ç–æ–±—ã –µ—ë –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å –ø–æ—Å—Ç–∞–≤—å –ª–∞–π–∫ –∏ –ø–æ–¥–ø–∏—à–∏—Å—å, –∞ —Ç–∞–∫–∂–µ –≤–≤–µ–¥–∏ –≤ —á–∞—Ç –∫–æ–º–∞–Ω–¥—É ILOVEYOU. –ö–æ–≥–¥–∞ –≤—ã –µ—ë –≤–≤–µ–¥—ë—Ç–µ —É –≤–∞—Å –Ω–∞ —ç–∫—Ä–∞–Ω–µ –ø–æ—è–≤–∏—Ç—Å—è –≤–æ—Ç —Ç–∞–∫–æ–π –∫–ª–æ—É–Ω. –°–∫—Ä–∏–ø—Ç –±—ã–ª —Å–æ–∑–¥–∞–Ω –æ–¥–Ω–∏–º –∏–∑ —Å–æ–∑–¥–∞—Ç–µ–ª–µ–π –†–æ–±–ª–æ–∫—Å–∞ –∏ —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ –≤ –∏–≥—Ä–∞—Ö, –≥–¥–µ –µ—Å—Ç—å –†–æ–±–ª–æ—Å –¥–µ–≤–µ–ª–æ–ø–µ—Ä —Å–µ—Ä–≤–∏—Å. –ß—Ç–æ–±—ã —É–±—Ä–∞—Ç—å —Å–∫—Ä–∏–º–µ—Ä –Ω—É–∂–Ω–æ –≤—Å–µ–≥–æ —Ç–æ –≤—ã–π—Ç–∏ –∏–∑ –∏–≥—Ä—ã, –Ω–æ –ª—É—á—à–µ –Ω–µ –ø—Ä–æ–≤–µ—Ä—è—è.',\n",
       " 'segments': [{'id': 0,\n",
       "   'seek': 0,\n",
       "   'start': 0.0,\n",
       "   'end': 2.56,\n",
       "   'text': ' –°–µ–π—á–∞—Å —è —Ç–µ–±–µ –ø–æ–∫–∞–∂—É —Å–µ–∫—Ä–µ—Ç–Ω—É—é –∫–æ–º–∞–Ω–¥—É –≤ –†–æ–±–ª–æ–∫—Å–µ.',\n",
       "   'tokens': [50365,\n",
       "    23590,\n",
       "    2552,\n",
       "    14656,\n",
       "    7240,\n",
       "    29294,\n",
       "    22869,\n",
       "    31242,\n",
       "    9882,\n",
       "    46180,\n",
       "    9031,\n",
       "    740,\n",
       "    6325,\n",
       "    2061,\n",
       "    693,\n",
       "    2637,\n",
       "    28340,\n",
       "    13,\n",
       "    50493],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.19574242953596444,\n",
       "   'compression_ratio': 1.8756906077348066,\n",
       "   'no_speech_prob': 0.005552722606807947},\n",
       "  {'id': 1,\n",
       "   'seek': 0,\n",
       "   'start': 2.56,\n",
       "   'end': 4.88,\n",
       "   'text': ' –ß—Ç–æ–±—ã –µ—ë –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å –ø–æ—Å—Ç–∞–≤—å –ª–∞–π–∫ –∏ –ø–æ–¥–ø–∏—à–∏—Å—å, –∞ —Ç–∞–∫–∂–µ',\n",
       "   'tokens': [50493,\n",
       "    36026,\n",
       "    18346,\n",
       "    30239,\n",
       "    26411,\n",
       "    28072,\n",
       "    678,\n",
       "    35475,\n",
       "    755,\n",
       "    1006,\n",
       "    4095,\n",
       "    1354,\n",
       "    9919,\n",
       "    10363,\n",
       "    11,\n",
       "    2559,\n",
       "    16584,\n",
       "    50609],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.19574242953596444,\n",
       "   'compression_ratio': 1.8756906077348066,\n",
       "   'no_speech_prob': 0.005552722606807947},\n",
       "  {'id': 2,\n",
       "   'seek': 0,\n",
       "   'start': 4.88,\n",
       "   'end': 6.8,\n",
       "   'text': ' –≤–≤–µ–¥–∏ –≤ —á–∞—Ç –∫–æ–º–∞–Ω–¥—É ILOVEYOU.',\n",
       "   'tokens': [50609,\n",
       "    740,\n",
       "    859,\n",
       "    31960,\n",
       "    740,\n",
       "    1358,\n",
       "    2134,\n",
       "    46180,\n",
       "    9031,\n",
       "    286,\n",
       "    20184,\n",
       "    7540,\n",
       "    56,\n",
       "    4807,\n",
       "    13,\n",
       "    50705],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.19574242953596444,\n",
       "   'compression_ratio': 1.8756906077348066,\n",
       "   'no_speech_prob': 0.005552722606807947},\n",
       "  {'id': 3,\n",
       "   'seek': 0,\n",
       "   'start': 6.8,\n",
       "   'end': 9.120000000000001,\n",
       "   'text': ' –ö–æ–≥–¥–∞ –≤—ã –µ—ë –≤–≤–µ–¥—ë—Ç–µ —É –≤–∞—Å –Ω–∞ —ç–∫—Ä–∞–Ω–µ –ø–æ—è–≤–∏—Ç—Å—è –≤–æ—Ç',\n",
       "   'tokens': [50705,\n",
       "    23128,\n",
       "    2840,\n",
       "    18346,\n",
       "    740,\n",
       "    32965,\n",
       "    2882,\n",
       "    5863,\n",
       "    1595,\n",
       "    10655,\n",
       "    1470,\n",
       "    41643,\n",
       "    387,\n",
       "    20011,\n",
       "    8254,\n",
       "    5505,\n",
       "    50821],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.19574242953596444,\n",
       "   'compression_ratio': 1.8756906077348066,\n",
       "   'no_speech_prob': 0.005552722606807947},\n",
       "  {'id': 4,\n",
       "   'seek': 0,\n",
       "   'start': 9.120000000000001,\n",
       "   'end': 10.46,\n",
       "   'text': ' —Ç–∞–∫–æ–π –∫–ª–æ—É–Ω.',\n",
       "   'tokens': [50821, 13452, 981, 4610, 12161, 13, 50888],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.19574242953596444,\n",
       "   'compression_ratio': 1.8756906077348066,\n",
       "   'no_speech_prob': 0.005552722606807947},\n",
       "  {'id': 5,\n",
       "   'seek': 0,\n",
       "   'start': 10.46,\n",
       "   'end': 12.200000000000001,\n",
       "   'text': ' –°–∫—Ä–∏–ø—Ç –±—ã–ª —Å–æ–∑–¥–∞–Ω –æ–¥–Ω–∏–º –∏–∑ —Å–æ–∑–¥–∞—Ç–µ–ª–µ–π –†–æ–±–ª–æ–∫—Å–∞',\n",
       "   'tokens': [50888,\n",
       "    22965,\n",
       "    50125,\n",
       "    403,\n",
       "    10059,\n",
       "    20247,\n",
       "    1416,\n",
       "    50096,\n",
       "    3943,\n",
       "    20247,\n",
       "    47573,\n",
       "    6325,\n",
       "    2061,\n",
       "    693,\n",
       "    2637,\n",
       "    22568,\n",
       "    50975],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.19574242953596444,\n",
       "   'compression_ratio': 1.8756906077348066,\n",
       "   'no_speech_prob': 0.005552722606807947},\n",
       "  {'id': 6,\n",
       "   'seek': 0,\n",
       "   'start': 12.200000000000001,\n",
       "   'end': 14.6,\n",
       "   'text': ' –∏ —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ –≤ –∏–≥—Ä–∞—Ö, –≥–¥–µ –µ—Å—Ç—å –†–æ–±–ª–æ—Å –¥–µ–≤–µ–ª–æ–ø–µ—Ä',\n",
       "   'tokens': [50975,\n",
       "    1006,\n",
       "    30162,\n",
       "    9008,\n",
       "    740,\n",
       "    20713,\n",
       "    25689,\n",
       "    11,\n",
       "    11418,\n",
       "    5640,\n",
       "    6325,\n",
       "    2061,\n",
       "    693,\n",
       "    1885,\n",
       "    20572,\n",
       "    1414,\n",
       "    3762,\n",
       "    1135,\n",
       "    51095],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.19574242953596444,\n",
       "   'compression_ratio': 1.8756906077348066,\n",
       "   'no_speech_prob': 0.005552722606807947},\n",
       "  {'id': 7,\n",
       "   'seek': 0,\n",
       "   'start': 14.6,\n",
       "   'end': 15.6,\n",
       "   'text': ' —Å–µ—Ä–≤–∏—Å.',\n",
       "   'tokens': [51095, 14490, 859, 3323, 13, 51145],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.19574242953596444,\n",
       "   'compression_ratio': 1.8756906077348066,\n",
       "   'no_speech_prob': 0.005552722606807947},\n",
       "  {'id': 8,\n",
       "   'seek': 0,\n",
       "   'start': 15.6,\n",
       "   'end': 17.6,\n",
       "   'text': ' –ß—Ç–æ–±—ã —É–±—Ä–∞—Ç—å —Å–∫—Ä–∏–º–µ—Ä –Ω—É–∂–Ω–æ –≤—Å–µ–≥–æ —Ç–æ –≤—ã–π—Ç–∏ –∏–∑ –∏–≥—Ä—ã,',\n",
       "   'tokens': [51145,\n",
       "    36026,\n",
       "    13853,\n",
       "    30260,\n",
       "    5239,\n",
       "    481,\n",
       "    12170,\n",
       "    12264,\n",
       "    15520,\n",
       "    4572,\n",
       "    42132,\n",
       "    3396,\n",
       "    3943,\n",
       "    36183,\n",
       "    11,\n",
       "    51245],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.19574242953596444,\n",
       "   'compression_ratio': 1.8756906077348066,\n",
       "   'no_speech_prob': 0.005552722606807947},\n",
       "  {'id': 9,\n",
       "   'seek': 0,\n",
       "   'start': 17.6,\n",
       "   'end': 18.8,\n",
       "   'text': ' –Ω–æ –ª—É—á—à–µ –Ω–µ –ø—Ä–æ–≤–µ—Ä—è—è.',\n",
       "   'tokens': [51245, 6035, 21569, 1725, 30901, 681, 681, 13, 51305],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.19574242953596444,\n",
       "   'compression_ratio': 1.8756906077348066,\n",
       "   'no_speech_prob': 0.005552722606807947}],\n",
       " 'language': 'ru'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "229c38a9-80b4-4ca1-b9d5-04a36f642d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.86 s, sys: 564 ms, total: 4.42 s\n",
      "Wall time: 1.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch_transcriptions = model.transcribe(audio_paths[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7b8e9a6-6ea2-4600-b441-f4f941ca9759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" I wanna spend some time with her Cause I can't get her through Type of bitch, I don't know who Falling in love\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_transcriptions['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008fd5e9-448a-4061-aa17-9edc43bbe879",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db640379-fce0-4187-8802-a58540d252cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.mlspace-anvlasov]",
   "language": "python",
   "name": "conda-env-.mlspace-anvlasov-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
