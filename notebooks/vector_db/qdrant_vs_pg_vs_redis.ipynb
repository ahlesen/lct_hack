{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import json\n",
    "import pgvector\n",
    "from typing import List, Optional\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "\n",
    "from pgvector.psycopg2 import register_vector\n",
    "import psycopg2\n",
    "\n",
    "# qdrant\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "from qdrant_client.http.models import PointStruct\n",
    "from qdrant_client.http.models import Filter, FieldCondition, MatchValue\n",
    "from qdrant_client.http.models import CollectionStatus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выполнить для vector_db, который был создан в pg: CREATE EXTENSION vector;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding():\n",
    "    return np.random.uniform(low=-1.0, high=1.0, size=100).tolist()\n",
    "\n",
    "def timer(func, kwargs):\n",
    "    start = time.time()\n",
    "    func(**kwargs)\n",
    "    end = time.time()\n",
    "    return end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n"
     ]
    }
   ],
   "source": [
    "count_of_store_ids_per_doc = 100\n",
    "fake_something = Faker()\n",
    "\n",
    "embs = np.random.uniform(low=-1.0, high=1.0, size=(1000000, 100))\n",
    "doc_ids = list(range(len(embs)))\n",
    "payloads = []\n",
    "\n",
    "for i in range(len(embs)):\n",
    "    payloads.append(\n",
    "        {\n",
    "            \"store_id\": [fake_something.random.randint(0, 100) for i in range(count_of_store_ids_per_doc)] ,\n",
    "            \"product_name\": \" \".join(fake_something.words()),\n",
    "            \"product_sku\": i,\n",
    "        }\n",
    "    )\n",
    "print(len(payloads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_id = 10\n",
    "embedding = embs[0].copy().astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PgVector - 0.3616370439529419"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(dbname='vector_db', \n",
    "                        host=\"localhost\", \n",
    "                        port=\"15432\", \n",
    "                        user=\"username\", \n",
    "                        password=\"password\",)\n",
    "\n",
    "conn.autocommit = True\n",
    "cursor = conn.cursor()\n",
    "# cursor.execute('CREATE EXTENSION IF NOT EXISTS vector')\n",
    "# cursor.execute('DROP TABLE IF EXISTS documents')\n",
    "register_vector(cursor)\n",
    "\n",
    "# cursor.execute('DROP TABLE IF EXISTS documents')\n",
    "# cursor.execute('CREATE TABLE documents (id bigserial PRIMARY KEY, product_name text, store_id integer[], embedding vector(100))')\n",
    "# cursor.execute('CREATE INDEX ON documents USING hnsw (embedding vector_l2_ops) WITH (m = 16, ef_construction = 100)')\n",
    "# cursor.execute('SET max_parallel_maintenance_workers=15;')\n",
    "# cursor.execute('SET max_parallel_workers=15;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for embedding, payload in zip(embs, payloads):\n",
    "#     store_id = payload[\"store_id\"]\n",
    "#     product_name = payload[\"product_name\"]\n",
    "#     product_sku = payload[\"product_sku\"]\n",
    "    \n",
    "#     cursor.execute('INSERT INTO documents (id, product_name, store_id, embedding) VALUES (%s, %s, %s, %s)', (product_sku, product_name, store_id, embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3616370439529419"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_k = 0\n",
    "for i in range(100):\n",
    "    time_k += timer(cursor.execute, {\"query\": \"\"\"SELECT \n",
    "                                                    id, \n",
    "                                                    product_name, \n",
    "                                                    store_id\n",
    "                                                FROM documents \n",
    "                                                WHERE %s = ANY(store_id)\n",
    "                                                ORDER BY embedding <=> %s\n",
    "                                                LIMIT 24\"\"\", \"vars\": (store_id, embedding)})\n",
    "    \n",
    "time_k / 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QDrant - 0.007430486679077149"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qdrant - pre-filter есть\n",
    "# Pgvector - pre-filter нет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QCollection:\n",
    "    def __init__(self, \n",
    "                 collection_name: str,\n",
    "                 url: str = \"localhost\", \n",
    "                 port: int = 6333):\n",
    "        self.client = QdrantClient(url=url, port=port, timeout=10000)\n",
    "        self.collection_name = collection_name\n",
    "        \n",
    "    def get_collection_info(self):\n",
    "        response = self.client.get_collection(collection_name=self.collection_name)\n",
    "        return response\n",
    "    \n",
    "    def create_collection(self, emb_dim: int = 100):\n",
    "        self.client.create_collection(\n",
    "            collection_name=self.collection_name,\n",
    "            vectors_config=VectorParams(size=emb_dim, distance=Distance.COSINE, on_disk=False),\n",
    "        )\n",
    "    \n",
    "    def delete_collection(self):\n",
    "        self.client.delete_collection(collection_name=self.collection_name)\n",
    "        \n",
    "    def payload_index_store_id(self):\n",
    "        self.client.create_payload_index(\n",
    "            collection_name=self.collection_name,\n",
    "            field_name=\"store_id\",\n",
    "            field_schema=\"integer\",\n",
    "        )\n",
    "\n",
    "        \n",
    "class QDocument:\n",
    "    def __init__(self, \n",
    "                 collection_name: str,\n",
    "                 url: str = \"localhost\", \n",
    "                 port: int = 6333,):\n",
    "        self.client = QdrantClient(url=url, port=port)\n",
    "        self.collection_name = collection_name\n",
    "        \n",
    "    def add_document(self, doc_id: int, embedding: List[float], metadata: dict):\n",
    "        # points with the same id will be overwritten when re-uploaded.\n",
    "        self.client.upsert(\n",
    "            collection_name=self.collection_name,\n",
    "            points=[\n",
    "                models.PointStruct(\n",
    "                    id=doc_id,\n",
    "                    payload=metadata,\n",
    "                    vector=embedding,\n",
    "                ),\n",
    "            ],\n",
    "        )\n",
    "    \n",
    "    def add_documents(self, doc_ids: List[int], embeddings: List[List[float]], metadata: List[dict]):\n",
    "        self.client.upsert(\n",
    "            collection_name=self.collection_name,\n",
    "            points=models.Batch(\n",
    "                ids=doc_ids,\n",
    "                payloads=metadata,\n",
    "                vectors=embeddings,\n",
    "            ),\n",
    "        )\n",
    "    \n",
    "    def delete_documents(self, doc_ids: List[int]):\n",
    "        self.client.delete(\n",
    "            collection_name=self.collection_name,\n",
    "            points_selector=models.PointIdsList(\n",
    "                points=doc_ids,\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "    def retrieve_documents(self, doc_ids: List[int]) -> dict:\n",
    "        response = self.client.retrieve(\n",
    "            collection_name=self.collection_name,\n",
    "            ids=doc_ids,\n",
    "            with_payload=True,\n",
    "            with_vectors=True,\n",
    "        )\n",
    "        return response\n",
    "    \n",
    "    def count_documents(self, store_id: int) -> dict:\n",
    "        response = self.client.count(\n",
    "            collection_name=self.collection_name,\n",
    "            count_filter=models.Filter(\n",
    "                must=[\n",
    "                    models.FieldCondition(key=\"store_id\", match=models.MatchValue(value=store_id)),\n",
    "                ]\n",
    "            ),\n",
    "            exact=True,\n",
    "        )\n",
    "        return response\n",
    "    \n",
    "    def search(self, embedding: List[float], store_id: int) -> dict:\n",
    "        response = self.client.search(\n",
    "            collection_name=self.collection_name,\n",
    "            query_filter=models.Filter(\n",
    "                must=[\n",
    "                    models.FieldCondition(\n",
    "                        key=\"store_id\",\n",
    "                        match=models.MatchValue(\n",
    "                            value=int(store_id),\n",
    "                        ),\n",
    "                    )\n",
    "                ]\n",
    "            ),\n",
    "            search_params=models.SearchParams(hnsw_ef=128, exact=False),\n",
    "            query_vector=embedding,\n",
    "            limit=24,\n",
    "            with_vectors=False,\n",
    "            with_payload=True,\n",
    "#             score_threshold=100,  # [0;1] cos\n",
    "        )\n",
    "        return response\n",
    "    \n",
    "    def batch_search(self):\n",
    "        ...\n",
    "        # https://qdrant.tech/documentation/concepts/search/#batch-search-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_collection = QCollection(collection_name=\"temp\")\n",
    "q_document = QDocument(collection_name=\"temp\")\n",
    "\n",
    "q_collection.delete_collection()\n",
    "q_collection.create_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('status', <CollectionStatus.GREEN: 'green'>)\n",
      " ('optimizer_status', <OptimizersStatusOneOf.OK: 'ok'>)\n",
      " ('vectors_count', 1000000)\n",
      " ('indexed_vectors_count', 985000)\n",
      " ('points_count', 1000000)\n",
      " ('segments_count', 6)\n",
      " ('config', CollectionConfig(params=CollectionParams(vectors=VectorParams(size=100, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=False), shard_number=1, sharding_method=None, replication_factor=1, write_consistency_factor=1, read_fan_out_factor=None, on_disk_payload=True, sparse_vectors=None), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=None, indexing_threshold=20000, flush_interval_sec=5, max_optimization_threads=1), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=None))\n",
      " ('payload_schema', {'store_id': PayloadIndexInfo(data_type=<PayloadSchemaType.INTEGER: 'integer'>, params=None, points=1000000)})\n",
      "\n",
      "1000000 1000000 1000000\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join([str(elem) + \"\\n\" for elem in list(q_collection.get_collection_info())]))\n",
    "print(len(payloads), len(embs), len(doc_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.8 s, sys: 9.3 s, total: 48.1 s\n",
      "Wall time: 10min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "step = 5000\n",
    "for t in range(0, len(doc_ids), step):\n",
    "    q_document.add_documents(doc_ids=doc_ids[t:step + t], \n",
    "                             embeddings=embs[t:step + t].tolist(), \n",
    "                             metadata=payloads[t:step + t])\n",
    "\n",
    "q_collection.payload_index_store_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007430486679077149"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_k = 0\n",
    "for i in range(100):\n",
    "    time_k += timer(q_document.search, {\"embedding\": embedding.tolist(), \"store_id\": store_id})\n",
    "    \n",
    "time_k / 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "import requests\n",
    "from redis.commands.search.field import (\n",
    "    NumericField,\n",
    "    TagField,\n",
    "    TextField,\n",
    "    VectorField,\n",
    ")\n",
    "from redis.commands.search.indexDefinition import IndexDefinition, IndexType\n",
    "from redis.commands.search.query import Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedisDB:\n",
    "    def __init__(self, collection_prefix=\"documents\", host=\"localhost\", port=8503):\n",
    "        self.client = redis.Redis(host=host, port=port, decode_responses=True)\n",
    "        self.collection_prefix = collection_prefix\n",
    "        print(f\"Host connection: {self.client.ping()}\")\n",
    "\n",
    "    def add_documents(self, doc_ids: List[int], embeddings: List[List[float]], metadata: List[dict]):\n",
    "        pipeline = self.client.pipeline()\n",
    "        for doc_id, payload, embedding in zip(doc_ids, metadata, embeddings):\n",
    "            payload[\"embedding\"] = embedding\n",
    "            payload[\"store_id\"] = list(map(str, payload[\"store_id\"])) # only for tags\n",
    "            redis_key = f\"{self.collection_prefix}:{doc_id}\"\n",
    "#             print(redis_key)\n",
    "            pipeline.json().set(redis_key, \"$\", payload)\n",
    "        executed = pipeline.execute()\n",
    "        print(f\"Success: {sum(executed) / len(executed)}.\")\n",
    "        \n",
    "    def count_documents(self) -> None:\n",
    "        keys = sorted(self.client.keys(f\"{self.collection_prefix}:*\"))\n",
    "        print(len(keys))\n",
    "    \n",
    "    def index_info(self, index_name: Optional[str] = None):\n",
    "        index_name = f\"idx:{self.collection_prefix if index_name is None else index_name}\" \n",
    "        return self.client.ft(index_name).info()\n",
    "        \n",
    "    def retrieve_document(self, doc_id: int) -> dict:\n",
    "        return self.client.json().get(f\"{self.collection_prefix}:{doc_id}\")\n",
    "    \n",
    "    def create_index(self, index_name: Optional[str] = None):\n",
    "        index_name = f\"idx:{self.collection_prefix if index_name is None else index_name}\" \n",
    "        # schema\n",
    "#         schema = (\n",
    "#             NumericField(\"$.store_id\", as_name=\"store_id\"),\n",
    "#             NumericField(\"$.product_sku\", as_name=\"product_sku\"),\n",
    "#             TextField(\"$.product_name\", no_stem=True, as_name=\"product_name\"),\n",
    "#             VectorField(\"$.embedding\",\n",
    "#                 \"HNSW\", {\n",
    "#                     \"TYPE\": \"FLOAT32\",\n",
    "#                     \"DIM\": 100,\n",
    "#                     \"DISTANCE_METRIC\": \"COSINE\",\n",
    "#                     \"M\": 16,\n",
    "#                     \"EF_CONSTRUCTION\": 100, # Up it to 200 as default redis LATER!\n",
    "#                 }, as_name=\"embedding\",\n",
    "#             ),\n",
    "#         )\n",
    "        \n",
    "        schema = (\n",
    "            TagField(\"$.store_id\", as_name=\"store_id\"), # swap to \"$.store_id.*\"\n",
    "            NumericField(\"$.product_sku\", as_name=\"product_sku\"),\n",
    "            TextField(\"$.product_name\", no_stem=True, as_name=\"product_name\"),\n",
    "            VectorField(\"$.embedding\",\n",
    "                \"HNSW\", {\n",
    "                    \"TYPE\": \"FLOAT32\",\n",
    "                    \"DIM\": 100,\n",
    "                    \"DISTANCE_METRIC\": \"COSINE\",\n",
    "                    \"M\": 16,\n",
    "                    \"EF_CONSTRUCTION\": 100, # Up it to 200 as default redis LATER!\n",
    "                }, as_name=\"embedding\",\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "        # index definition\n",
    "        definition = IndexDefinition(prefix=[f\"{self.collection_prefix}:\"], index_type=IndexType.JSON)\n",
    "        # create index\n",
    "        self.client.ft(index_name).create_index(fields=schema, definition=definition)\n",
    "    \n",
    "    def search(self, query: Query, store_id: int, query_vector: bytes, index_name: Optional[str] = None):\n",
    "        index_name = f\"idx:{self.collection_prefix if index_name is None else index_name}\"\n",
    "        response = self.client.ft(index_name).search(query, {\"query_vector\": query_vector, \"store_id\": store_id}).docs\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### store_id as NumericField - 0.0011725187301635741"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Host connection: True\n"
     ]
    }
   ],
   "source": [
    "# rediska.client.flushall()\n",
    "rediska = RedisDB()\n",
    "# 16 56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rediska.add_documents(doc_ids=doc_ids, embeddings=embs.tolist(), metadata=payloads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n"
     ]
    }
   ],
   "source": [
    "rediska.count_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "rediska.create_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index_name': 'idx:documents',\n",
       " 'index_options': [],\n",
       " 'index_definition': ['key_type',\n",
       "  'JSON',\n",
       "  'prefixes',\n",
       "  ['documents:'],\n",
       "  'default_score',\n",
       "  '1'],\n",
       " 'attributes': [['identifier',\n",
       "   '$.store_id',\n",
       "   'attribute',\n",
       "   'store_id',\n",
       "   'type',\n",
       "   'NUMERIC'],\n",
       "  ['identifier',\n",
       "   '$.product_sku',\n",
       "   'attribute',\n",
       "   'product_sku',\n",
       "   'type',\n",
       "   'NUMERIC'],\n",
       "  ['identifier',\n",
       "   '$.product_name',\n",
       "   'attribute',\n",
       "   'product_name',\n",
       "   'type',\n",
       "   'TEXT',\n",
       "   'WEIGHT',\n",
       "   '1',\n",
       "   'NOSTEM'],\n",
       "  ['identifier',\n",
       "   '$.embedding',\n",
       "   'attribute',\n",
       "   'embedding',\n",
       "   'type',\n",
       "   'VECTOR',\n",
       "   'algorithm',\n",
       "   'HNSW',\n",
       "   'data_type',\n",
       "   'FLOAT32',\n",
       "   'dim',\n",
       "   100,\n",
       "   'distance_metric',\n",
       "   'COSINE',\n",
       "   'M',\n",
       "   16,\n",
       "   'ef_construction',\n",
       "   200]],\n",
       " 'num_docs': '1000000',\n",
       " 'max_doc_id': '1000000',\n",
       " 'num_terms': '942',\n",
       " 'num_records': '104907091',\n",
       " 'inverted_sz_mb': '266.12841796875',\n",
       " 'vector_index_sz_mb': '720.0846557617188',\n",
       " 'total_inverted_index_blocks': '1035000',\n",
       " 'offset_vectors_sz_mb': '2.775178909301758',\n",
       " 'doc_table_size_mb': '78.09532928466797',\n",
       " 'sortable_values_size_mb': '0',\n",
       " 'key_table_size_mb': '29.963886260986328',\n",
       " 'geoshapes_sz_mb': '0',\n",
       " 'records_per_doc_avg': '104.90708923339844',\n",
       " 'bytes_per_record_avg': '2.6600286960601807',\n",
       " 'offsets_per_term_avg': '0.027738697826862335',\n",
       " 'offset_bits_per_record_avg': '8',\n",
       " 'hash_indexing_failures': '0',\n",
       " 'total_indexing_time': '760636.25',\n",
       " 'indexing': '0',\n",
       " 'percent_indexed': '1',\n",
       " 'number_of_uses': 8,\n",
       " 'cleaning': 0,\n",
       " 'gc_stats': ['bytes_collected',\n",
       "  '0',\n",
       "  'total_ms_run',\n",
       "  '0',\n",
       "  'total_cycles',\n",
       "  '0',\n",
       "  'average_cycle_time_ms',\n",
       "  'nan',\n",
       "  'last_run_time_ms',\n",
       "  '0',\n",
       "  'gc_numeric_trees_missed',\n",
       "  '0',\n",
       "  'gc_blocks_denied',\n",
       "  '0'],\n",
       " 'cursor_stats': ['global_idle',\n",
       "  0,\n",
       "  'global_total',\n",
       "  0,\n",
       "  'index_capacity',\n",
       "  128,\n",
       "  'index_total',\n",
       "  0],\n",
       " 'dialect_stats': ['dialect_1',\n",
       "  0,\n",
       "  'dialect_2',\n",
       "  0,\n",
       "  'dialect_3',\n",
       "  0,\n",
       "  'dialect_4',\n",
       "  0],\n",
       " 'Index Errors': ['indexing failures',\n",
       "  0,\n",
       "  'last indexing error',\n",
       "  'N/A',\n",
       "  'last indexing error key',\n",
       "  'N/A'],\n",
       " 'field statistics': [['identifier',\n",
       "   '$.store_id',\n",
       "   'attribute',\n",
       "   'store_id',\n",
       "   'Index Errors',\n",
       "   ['indexing failures',\n",
       "    0,\n",
       "    'last indexing error',\n",
       "    'N/A',\n",
       "    'last indexing error key',\n",
       "    'N/A']],\n",
       "  ['identifier',\n",
       "   '$.product_sku',\n",
       "   'attribute',\n",
       "   'product_sku',\n",
       "   'Index Errors',\n",
       "   ['indexing failures',\n",
       "    0,\n",
       "    'last indexing error',\n",
       "    'N/A',\n",
       "    'last indexing error key',\n",
       "    'N/A']],\n",
       "  ['identifier',\n",
       "   '$.product_name',\n",
       "   'attribute',\n",
       "   'product_name',\n",
       "   'Index Errors',\n",
       "   ['indexing failures',\n",
       "    0,\n",
       "    'last indexing error',\n",
       "    'N/A',\n",
       "    'last indexing error key',\n",
       "    'N/A']],\n",
       "  ['identifier',\n",
       "   '$.embedding',\n",
       "   'attribute',\n",
       "   'embedding',\n",
       "   'Index Errors',\n",
       "   ['indexing failures',\n",
       "    0,\n",
       "    'last indexing error',\n",
       "    'N/A',\n",
       "    'last indexing error key',\n",
       "    'N/A']]]}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rediska.index_info()  # wait till 'percent_indexed': '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0011725187301635741"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = (\n",
    "    Query('(@store_id:[$store_id $store_id])=>[KNN 24 @embedding $query_vector AS vector_score]')\n",
    "     .sort_by('vector_score')\n",
    "     .return_fields('vector_score', 'store_id', 'product_name', 'product_sku')\n",
    "     .dialect(2)\n",
    "     .paging(0, 24)\n",
    ")\n",
    "\n",
    "time_k = 0\n",
    "for i in range(100):\n",
    "    time_k += timer(rediska.search, {\"query\": query, \"query_vector\": embs[0].astype(np.float32).tobytes(), \"store_id\": store_id})\n",
    "    \n",
    "time_k / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\n",
    "    Query('(@store_id:[$store_id $store_id])=>[KNN 24 @embedding $query_vector AS vector_score]')\n",
    "     .sort_by('vector_score')\n",
    "     .return_fields('vector_score', 'store_id', 'product_name', 'product_sku')\n",
    "     .dialect(2)\n",
    "     .paging(0, 24)\n",
    ")\n",
    "\n",
    "response = rediska.search(query=query, store_id=store_id, query_vector=embs[0].astype(np.float32).tobytes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(len(response))\n",
    "for doc in response:\n",
    "    print(f\"{store_id},\" in doc.store_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### store_id as TagField - 0.001410665512084961"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n"
     ]
    }
   ],
   "source": [
    "count_of_store_ids_per_doc = 100\n",
    "fake_something = Faker()\n",
    "\n",
    "embs = np.random.uniform(low=-1.0, high=1.0, size=(1000000, 100))\n",
    "doc_ids = list(range(len(embs)))\n",
    "payloads = []\n",
    "\n",
    "for i in range(len(embs)):\n",
    "    payloads.append(\n",
    "        {\n",
    "            \"store_id\": list(set([fake_something.random.randint(0, 100) for i in range(count_of_store_ids_per_doc)])),\n",
    "            \"product_name\": \" \".join(fake_something.words()),\n",
    "            \"product_sku\": i,\n",
    "        }\n",
    "    )\n",
    "print(len(payloads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1278\n"
     ]
    }
   ],
   "source": [
    "unique_stores = set()\n",
    "for payload in payloads:\n",
    "    unique_stores.update(set(payload[\"store_id\"]))\n",
    "    break\n",
    "print(len(unique_stores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Host connection: True\n"
     ]
    }
   ],
   "source": [
    "rediska.client.flushall()\n",
    "rediska = RedisDB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: 1.0.\n"
     ]
    }
   ],
   "source": [
    "rediska.add_documents(doc_ids=doc_ids, embeddings=embs.tolist(), metadata=payloads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n"
     ]
    }
   ],
   "source": [
    "rediska.count_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "rediska.create_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rediska.index_info()[\"percent_indexed\"]  # wait till 'percent_indexed': '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\n",
    "    Query('(@store_id:{$store_id})=>[KNN 24 @embedding $query_vector AS vector_score]')\n",
    "     .sort_by('vector_score')\n",
    "     .return_fields('vector_score', 'store_id', 'product_name', 'product_sku')\n",
    "     .dialect(2) # dialect 3 for .*\n",
    "     .paging(0, 24)\n",
    ")\n",
    "\n",
    "response = rediska.search(query=query, store_id=store_id, query_vector=embs[0].astype(np.float32).tobytes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(len(response))\n",
    "for doc in response:\n",
    "    print(f\"{store_id}\" in json.loads(doc.store_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001267526149749756"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = (\n",
    "    Query('(@store_id:{$store_id})=>[KNN 24 @embedding $query_vector AS vector_score]')\n",
    "     .sort_by('vector_score')\n",
    "     .return_fields('vector_score', 'store_id', 'product_name', 'product_sku')\n",
    "     .dialect(2) # dialect 3 for .*\n",
    "     .paging(0, 24)\n",
    ")\n",
    "\n",
    "time_k = 0\n",
    "for i in range(100):\n",
    "    time_k += timer(rediska.search, {\"query\": query, \"query_vector\": embs[0].astype(np.float32).tobytes(), \"store_id\": store_id})\n",
    "    \n",
    "time_k / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index_name': 'idx:documents',\n",
       " 'index_options': [],\n",
       " 'index_definition': ['key_type',\n",
       "  'JSON',\n",
       "  'prefixes',\n",
       "  ['documents:'],\n",
       "  'default_score',\n",
       "  '1'],\n",
       " 'attributes': [['identifier',\n",
       "   '$.store_id',\n",
       "   'attribute',\n",
       "   'store_id',\n",
       "   'type',\n",
       "   'TAG',\n",
       "   'SEPARATOR',\n",
       "   ','],\n",
       "  ['identifier',\n",
       "   '$.product_sku',\n",
       "   'attribute',\n",
       "   'product_sku',\n",
       "   'type',\n",
       "   'NUMERIC'],\n",
       "  ['identifier',\n",
       "   '$.product_name',\n",
       "   'attribute',\n",
       "   'product_name',\n",
       "   'type',\n",
       "   'TEXT',\n",
       "   'WEIGHT',\n",
       "   '1',\n",
       "   'NOSTEM'],\n",
       "  ['identifier',\n",
       "   '$.embedding',\n",
       "   'attribute',\n",
       "   'embedding',\n",
       "   'type',\n",
       "   'VECTOR',\n",
       "   'algorithm',\n",
       "   'HNSW',\n",
       "   'data_type',\n",
       "   'FLOAT32',\n",
       "   'dim',\n",
       "   100,\n",
       "   'distance_metric',\n",
       "   'COSINE',\n",
       "   'M',\n",
       "   16,\n",
       "   'ef_construction',\n",
       "   100]],\n",
       " 'num_docs': '1000000',\n",
       " 'max_doc_id': '1000000',\n",
       " 'num_terms': '942',\n",
       " 'num_records': '5907234',\n",
       " 'inverted_sz_mb': '83.89717102050781',\n",
       " 'vector_index_sz_mb': '723.476318359375',\n",
       " 'total_inverted_index_blocks': '103520',\n",
       " 'offset_vectors_sz_mb': '2.7753477096557617',\n",
       " 'doc_table_size_mb': '78.09532928466797',\n",
       " 'sortable_values_size_mb': '0',\n",
       " 'key_table_size_mb': '29.963886260986328',\n",
       " 'geoshapes_sz_mb': '0',\n",
       " 'records_per_doc_avg': '5.907234191894531',\n",
       " 'bytes_per_record_avg': '14.892343521118164',\n",
       " 'offsets_per_term_avg': '0.49264392256736755',\n",
       " 'offset_bits_per_record_avg': '8',\n",
       " 'hash_indexing_failures': '0',\n",
       " 'total_indexing_time': '452345.65625',\n",
       " 'indexing': '0',\n",
       " 'percent_indexed': '1',\n",
       " 'number_of_uses': 110,\n",
       " 'cleaning': 0,\n",
       " 'gc_stats': ['bytes_collected',\n",
       "  '0',\n",
       "  'total_ms_run',\n",
       "  '0',\n",
       "  'total_cycles',\n",
       "  '0',\n",
       "  'average_cycle_time_ms',\n",
       "  'nan',\n",
       "  'last_run_time_ms',\n",
       "  '0',\n",
       "  'gc_numeric_trees_missed',\n",
       "  '0',\n",
       "  'gc_blocks_denied',\n",
       "  '0'],\n",
       " 'cursor_stats': ['global_idle',\n",
       "  0,\n",
       "  'global_total',\n",
       "  0,\n",
       "  'index_capacity',\n",
       "  128,\n",
       "  'index_total',\n",
       "  0],\n",
       " 'dialect_stats': ['dialect_1',\n",
       "  0,\n",
       "  'dialect_2',\n",
       "  1,\n",
       "  'dialect_3',\n",
       "  1,\n",
       "  'dialect_4',\n",
       "  0],\n",
       " 'Index Errors': ['indexing failures',\n",
       "  0,\n",
       "  'last indexing error',\n",
       "  'N/A',\n",
       "  'last indexing error key',\n",
       "  'N/A'],\n",
       " 'field statistics': [['identifier',\n",
       "   '$.store_id',\n",
       "   'attribute',\n",
       "   'store_id',\n",
       "   'Index Errors',\n",
       "   ['indexing failures',\n",
       "    0,\n",
       "    'last indexing error',\n",
       "    'N/A',\n",
       "    'last indexing error key',\n",
       "    'N/A']],\n",
       "  ['identifier',\n",
       "   '$.product_sku',\n",
       "   'attribute',\n",
       "   'product_sku',\n",
       "   'Index Errors',\n",
       "   ['indexing failures',\n",
       "    0,\n",
       "    'last indexing error',\n",
       "    'N/A',\n",
       "    'last indexing error key',\n",
       "    'N/A']],\n",
       "  ['identifier',\n",
       "   '$.product_name',\n",
       "   'attribute',\n",
       "   'product_name',\n",
       "   'Index Errors',\n",
       "   ['indexing failures',\n",
       "    0,\n",
       "    'last indexing error',\n",
       "    'N/A',\n",
       "    'last indexing error key',\n",
       "    'N/A']],\n",
       "  ['identifier',\n",
       "   '$.embedding',\n",
       "   'attribute',\n",
       "   'embedding',\n",
       "   'Index Errors',\n",
       "   ['indexing failures',\n",
       "    0,\n",
       "    'last indexing error',\n",
       "    'N/A',\n",
       "    'last indexing error key',\n",
       "    'N/A']]]}"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rediska.index_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
